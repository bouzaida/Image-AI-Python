{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-7\n",
    "### What is next?\n",
    "#### Deep CNN models with big filters\n",
    "\n",
    "**Description:**  \n",
    "*Train designed deep CNN models with big filters in the 1st convolutional layer  \n",
    "Create video to visualize training process of the filters*  \n",
    "\n",
    "**File:** *what_is_next_part_1.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Open preprocessed dataset  \n",
    "**--> Step 2:** Load saved 1st deep CNN model  \n",
    "**--> Step 3:** Train updated model on one preprocessed dataset  \n",
    "**--> Step 4:** Predict with test dataset  \n",
    "**--> Step 5:** Visualize filters of convolutional layer  \n",
    "**--> Step 6:** Convert intermediate weights into images  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Images of intermediate weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.models import load_model, model_from_json\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section4'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section5' with designed models\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section5'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section5'\n",
    "full_path_to_Section5 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining function to plot filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to collect all filters in one grid\n",
    "def convert_to_grid(x_input):\n",
    "    # Getting shape values of the input\n",
    "    number, height, width, channels = x_input.shape\n",
    "    \n",
    "    # Calculating grid size, number of rows and columns\n",
    "    # Every element of the grid represents one filter\n",
    "    grid_size = int(np.ceil(np.sqrt(number)))\n",
    "    \n",
    "    # Calculating height and width of entire grid according to\n",
    "    # number of pixels along X and Y axes\n",
    "    # Adding extra pixels to be as gaps in between filters\n",
    "    grid_height = height * grid_size + (grid_size - 1)\n",
    "    grid_width = width * grid_size + (grid_size - 1)\n",
    "    \n",
    "    # Creating Numpy array for the grid\n",
    "    # Filling all grid pixels with white colour\n",
    "    grid = np.zeros((grid_height, grid_width, channels)) + 255\n",
    "    \n",
    "    # Defining index for current filter\n",
    "    n = 0\n",
    "    \n",
    "    # Defining positions for current filter along Y axis of the grid\n",
    "    y_min, y_max = 0, height\n",
    "    \n",
    "    # Iterating all grid elements and filling them with filters' values\n",
    "    # Iterating every row of the grid elements\n",
    "    for y in range(grid_size):\n",
    "        # Defining positions for current filter along X axis of the grid\n",
    "        x_min, x_max = 0, width\n",
    "        \n",
    "        # Iterating every column of the current row\n",
    "        for x in range(grid_size):\n",
    "            # Checking if current index of the filter is less \n",
    "            # than total number of filters\n",
    "            if n < number:\n",
    "                # Getting current filter from the input\n",
    "                filter_current = x_input[n]\n",
    "                \n",
    "                # Extracting minimum and maximum values of current filter\n",
    "                f_min, f_max = np.min(filter_current), np.max(filter_current)\n",
    "                \n",
    "                # Scaling filter's values to be in range [0, 255]\n",
    "                # Assigning scaled values to appropriate positions in the grid\n",
    "                grid[y_min:y_max, x_min:x_max] = 255.0 * (filter_current - f_min) / (f_max - f_min)\n",
    "                \n",
    "                # Increasing index of current filter\n",
    "                n += 1\n",
    "            \n",
    "            # Moving positions for current filter along X axis of the grid\n",
    "            # Adding 1 pixel to be as gap in between filters\n",
    "            x_min += width + 1\n",
    "            x_max += width + 1\n",
    "        \n",
    "        # Moving positions for current filter along Y axis of the grid\n",
    "        # Adding 1 pixel to be as gap in between filters\n",
    "        y_min += height + 1\n",
    "        y_max += height + 1\n",
    "        \n",
    "    # Returning grid filled with filters\n",
    "    return grid\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Function to create a grid is successfully defined')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved custom dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + \n",
    "               'custom' + '/' + \n",
    "               'dataset_custom_rgb_255_mean.hdf5', 'r') as f:\n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 5)\n",
    "y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 2: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb = load_model(full_path_to_Section5 + '/' + 'custom' + '/' + 'model_1_custom_rgb.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning new spatial dimension for the filters of the 1st convolutional layer\n",
    "model_rgb.layers[0].kernel_size = (32, 32)\n",
    "\n",
    "\n",
    "# Re-loading model and initializing it by new structure\n",
    "model_rgb = model_from_json(model_rgb.to_json())\n",
    "\n",
    "\n",
    "# Re-compiling model\n",
    "model_rgb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully re-loaded')\n",
    "print('Model is successfully re-compiled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing filepath to save best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "best_weights_filepath = 'custom' + '/' + 'w_1_custom_rgb_255_mean.h5'\n",
    "\n",
    "\n",
    "# Defining schedule to save best weights\n",
    "best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                               save_weights_only=True,                                   \n",
    "                               monitor='val_accuracy',\n",
    "                               mode='max',\n",
    "                               save_best_only=True,\n",
    "                               period=1,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save best weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schedule to save intermediate weights\n",
    "class CustomCallback(Callback):\n",
    "    # Constructor of the class\n",
    "    def __init__(self):\n",
    "        # Defining variable to be as a part of filename\n",
    "        self.filename = 0\n",
    "        \n",
    "    # Function that is called at the end of every batch\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Checking if it is every 10th batch\n",
    "        if batch % 10 == 0:\n",
    "            # Preparing filepath to save intermediate weights\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            intermediate_weights_filepath = 'custom' + '/' + \\\n",
    "                                            'intermediate' + '/' + \\\n",
    "                                            '{0:04d}'.format(self.filename) + \\\n",
    "                                            '_w_1_custom_rgb_255_mean.h5'\n",
    "            \n",
    "            # Getting weights only for the first convolutional layer\n",
    "            weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "            # Saving obtained weights into new HDF5 binary file\n",
    "            # Initiating File object\n",
    "            # Creating file with current name\n",
    "            # Opening it in writing mode by 'w'\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                # Calling method to create dataset of given shape and type\n",
    "                # Saving Numpy array with weights from the first layer\n",
    "                f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "            # Increasing variable to be as a part of the next filename\n",
    "            self.filename += 1\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save intermediate weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model_rgb.fit(x_train, y_train,\n",
    "                  batch_size=50,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_validation, y_validation),\n",
    "                  callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 4: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and assigning best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb.load_weights('custom' + '/' + 'w_1_custom_rgb_255_mean.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Best weights for 1st RGB model are loaded and assigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RGB model\n",
    "temp = model_rgb.predict(x_test)\n",
    "        \n",
    "\n",
    "# Check point\n",
    "# Showing prediction shape and scores\n",
    "print('prediction shape  :', temp.shape)  # (278, 5)\n",
    "print('prediction scores :', temp[0])  # 5 score numbers\n",
    "print()\n",
    "\n",
    "    \n",
    "# Getting indexes of maximum values along specified axis\n",
    "temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "# Check point\n",
    "# Showing prediction shape after convertion\n",
    "# Showing predicted and correct indexes of classes\n",
    "print('prediction shape  :', temp.shape)  # (278,)\n",
    "print('predicted indexes :', temp[0:10])\n",
    "print('correct indexes   :', y_test[:10])\n",
    "print()\n",
    "\n",
    "\n",
    "# Calculating accuracy\n",
    "# We compare predicted class with correct class for all input images\n",
    "# By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "# By function 'np.mean' we calculate mean value:\n",
    "# all_True / (all_True + all_False)\n",
    "accuracy = np.mean(temp == y_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing calculated accuracy\n",
    "print('Testing accuracy  : {0:.5f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 5: Visualizing filters of convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the weights from entire model\n",
    "w = model_rgb.get_weights()\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n",
    "\n",
    "\n",
    "# Moving last dimension to the first position\n",
    "# Reshaping Numpy array from\n",
    "# (height, width, channels, number) to\n",
    "# (number, height, width, channels)\n",
    "w[0] = w[0].transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Converting weights of convolutional layer into a grid\n",
    "grid = convert_to_grid(w[0])\n",
    "\n",
    "\n",
    "# Plotting the grid\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "\n",
    "\n",
    "# Switching off axes\n",
    "# Setting size of the plot (width, height) in inches\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 6: Converting intermediate weights into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterating all saved intermediate weights\n",
    "for i in range(350):\n",
    "    # Opening saved binary file with intermediate weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File('custom/intermediate/' + '{0:04d}'.format(i) + \\\n",
    "                   '_w_1_custom_rgb_255_mean.h5', 'r') as f:\n",
    "        # Extracting saved array with intermediate weights by appropriate key\n",
    "        # Saving them into new variable\n",
    "        weights = f['weights_layer_0']  # HDF5 dataset\n",
    "\n",
    "        # Converting them into Numpy array\n",
    "        weights = np.array(weights)  # Numpy arrays\n",
    "\n",
    "    \n",
    "    # Moving last dimension to the first position\n",
    "    # Reshaping Numpy array from\n",
    "    # (height, width, channels, number) to\n",
    "    # (number, height, width, channels)\n",
    "    weights = weights.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    \n",
    "    # Converting weights of convolutional layer into a grid\n",
    "    grid = convert_to_grid(weights)\n",
    "    \n",
    "    \n",
    "    # Plotting the grid\n",
    "    plt.imshow(grid.astype('uint8'))\n",
    "    \n",
    "    \n",
    "    # Switching off axes\n",
    "    # Setting size of the plot (width, height) in inches\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    \n",
    "    \n",
    "    # Saving the plot into image file\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    plt.savefig('custom/intermediate/images/' + '{0:04d}'.format(i) + '.png', dpi=300)\n",
    "    \n",
    "    \n",
    "    # Closing the plot\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('{0:04d}.png is created'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + \n",
    "               'cifar10' + '/' + \n",
    "               'dataset_cifar10_rgb_255_mean.hdf5', 'r') as f:\n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 2: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb = load_model(full_path_to_Section5 + '/' + 'cifar10' + '/' + 'model_1_cifar10_rgb.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning new spatial dimension for the filters of the 1st convolutional layer\n",
    "model_rgb.layers[0].kernel_size = (16, 16)\n",
    "\n",
    "\n",
    "# Re-loading model and initializing it by new structure\n",
    "model_rgb = model_from_json(model_rgb.to_json())\n",
    "\n",
    "\n",
    "# Re-compiling model\n",
    "model_rgb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully re-loaded')\n",
    "print('Model is successfully re-compiled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing filepath to save best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "best_weights_filepath = 'cifar10' + '/' + 'w_1_cifar10_rgb_255_mean.h5'\n",
    "\n",
    "\n",
    "# Defining schedule to save best weights\n",
    "best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                               save_weights_only=True,\n",
    "                               monitor='val_accuracy',\n",
    "                               mode='max',\n",
    "                               save_best_only=True,\n",
    "                               period=1,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save best weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schedule to save intermediate weights\n",
    "class CustomCallback(Callback):\n",
    "    # Constructor of the class\n",
    "    def __init__(self):\n",
    "        # Defining variable to be as a part of filename\n",
    "        self.filename = 0\n",
    "        \n",
    "    # Function that is called at the end of every batch\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Checking if it is every 100th batch\n",
    "        if batch % 100 == 0:\n",
    "            # Preparing filepath to save intermediate weights\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            intermediate_weights_filepath = 'cifar10' + '/' + \\\n",
    "                                            'intermediate' + '/' + \\\n",
    "                                            '{0:04d}'.format(self.filename) + \\\n",
    "                                            '_w_1_cifar10_rgb_255_mean.h5'\n",
    "            \n",
    "            # Getting weights only for the first convolutional layer\n",
    "            weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "            # Saving obtained weights into new HDF5 binary file\n",
    "            # Initiating File object\n",
    "            # Creating file with current name\n",
    "            # Opening it in writing mode by 'w'\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                # Calling method to create dataset of given shape and type\n",
    "                # Saving Numpy array with weights from the first layer\n",
    "                f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "            # Increasing variable to be as a part of the next filename\n",
    "            self.filename += 1\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save intermediate weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model_rgb.fit(x_train, y_train,\n",
    "                  batch_size=50,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_validation, y_validation),\n",
    "                  callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 4: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and assigning best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb.load_weights('cifar10' + '/' + 'w_1_cifar10_rgb_255_mean.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Best weights for 1st RGB model are loaded and assigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RGB model\n",
    "temp = model_rgb.predict(x_test)\n",
    "        \n",
    "\n",
    "# Check point\n",
    "# Showing prediction shape and scores\n",
    "print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "print()\n",
    "\n",
    "    \n",
    "# Getting indexes of maximum values along specified axis\n",
    "temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "# Check point\n",
    "# Showing prediction shape after convertion\n",
    "# Showing predicted and correct indexes of classes\n",
    "print('prediction shape  :', temp.shape)  # (10000,)\n",
    "print('predicted indexes :', temp[0:10])\n",
    "print('correct indexes   :', y_test[:10])\n",
    "print()\n",
    "\n",
    "\n",
    "# Calculating accuracy\n",
    "# We compare predicted class with correct class for all input images\n",
    "# By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "# By function 'np.mean' we calculate mean value:\n",
    "# all_True / (all_True + all_False)\n",
    "accuracy = np.mean(temp == y_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing calculated accuracy\n",
    "print('Testing accuracy  : {0:.5f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 5: Visualizing filters of convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the weights from entire model\n",
    "w = model_rgb.get_weights()\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n",
    "\n",
    "\n",
    "# Moving last dimension to the first position\n",
    "# Reshaping Numpy array from\n",
    "# (height, width, channels, number) to\n",
    "# (number, height, width, channels)\n",
    "w[0] = w[0].transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Converting weights of convolutional layer into a grid\n",
    "grid = convert_to_grid(w[0])\n",
    "\n",
    "\n",
    "# Plotting the grid\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "\n",
    "\n",
    "# Switching off axes\n",
    "# Setting size of the plot (width, height) in inches\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 6: Converting intermediate weights into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterating all saved intermediate weights\n",
    "for i in range(450):\n",
    "    # Opening saved binary file with intermediate weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File('cifar10/intermediate/' + '{0:04d}'.format(i) + \\\n",
    "                   '_w_1_cifar10_rgb_255_mean.h5', 'r') as f:\n",
    "        # Extracting saved array with intermediate weights by appropriate key\n",
    "        # Saving them into new variable\n",
    "        weights = f['weights_layer_0']  # HDF5 dataset\n",
    "\n",
    "        # Converting them into Numpy array\n",
    "        weights = np.array(weights)  # Numpy arrays\n",
    "\n",
    "    \n",
    "    # Moving last dimension to the first position\n",
    "    # Reshaping Numpy array from\n",
    "    # (height, width, channels, number) to\n",
    "    # (number, height, width, channels)\n",
    "    weights = weights.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    \n",
    "    # Converting weights of convolutional layer into a grid\n",
    "    grid = convert_to_grid(weights)\n",
    "    \n",
    "    \n",
    "    # Plotting the grid\n",
    "    plt.imshow(grid.astype('uint8'))\n",
    "    \n",
    "    \n",
    "    # Switching off axes\n",
    "    # Setting size of the plot (width, height) in inches\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    \n",
    "    \n",
    "    # Saving the plot into image file\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    plt.savefig('cifar10/intermediate/images/' + '{0:04d}'.format(i) + '.png', dpi=300)\n",
    "    \n",
    "    \n",
    "    # Closing the plot\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('{0:04d}.png is created'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved MNIST dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + \n",
    "               'mnist' + '/' + \n",
    "               'dataset_mnist_gray_255_mean.hdf5', 'r') as f:\n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 2: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_gray = load_model(full_path_to_Section5 + '/' + 'mnist' + '/' + 'model_1_mnist_gray.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_gray.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_gray.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning new spatial dimension for the filters of the 1st convolutional layer\n",
    "model_gray.layers[0].kernel_size = (14, 14)\n",
    "\n",
    "\n",
    "# Re-loading model and initializing it by new structure\n",
    "model_gray = model_from_json(model_gray.to_json())\n",
    "\n",
    "\n",
    "# Re-compiling model\n",
    "model_gray.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully re-loaded')\n",
    "print('Model is successfully re-compiled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_gray.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_gray.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing filepath to save best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "best_weights_filepath = 'mnist' + '/' + 'w_1_mnist_gray_255_mean.h5'\n",
    "\n",
    "\n",
    "# Defining schedule to save best weights\n",
    "best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                               save_weights_only=True,                                   \n",
    "                               monitor='val_accuracy',\n",
    "                               mode='max',\n",
    "                               save_best_only=True,\n",
    "                               period=1,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save best weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schedule to save intermediate weights\n",
    "class CustomCallback(Callback):\n",
    "    # Constructor of the class\n",
    "    def __init__(self):\n",
    "        # Defining variable to be as a part of filename\n",
    "        self.filename = 0\n",
    "        \n",
    "    # Function that is called at the end of every batch\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Checking if it is every 100th batch\n",
    "        if batch % 100 == 0:\n",
    "            # Preparing filepath to save intermediate weights\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            intermediate_weights_filepath = 'mnist' + '/' + \\\n",
    "                                            'intermediate' + '/' + \\\n",
    "                                            '{0:04d}'.format(self.filename) + \\\n",
    "                                            '_w_1_mnist_gray_255_mean.h5'\n",
    "            \n",
    "            # Getting weights only for the first convolutional layer\n",
    "            weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "            # Saving obtained weights into new HDF5 binary file\n",
    "            # Initiating File object\n",
    "            # Creating file with current name\n",
    "            # Opening it in writing mode by 'w'\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                # Calling method to create dataset of given shape and type\n",
    "                # Saving Numpy array with weights from the first layer\n",
    "                f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "            # Increasing variable to be as a part of the next filename\n",
    "            self.filename += 1\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save intermediate weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model_gray.fit(x_train, y_train,\n",
    "                   batch_size=50,\n",
    "                   epochs=epochs,\n",
    "                   validation_data=(x_validation, y_validation),\n",
    "                   callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                   verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 4: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and assigning best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_gray.load_weights('mnist' + '/' + 'w_1_mnist_gray_255_mean.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Best weights for 1st GRAY model are loaded and assigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing GRAY model\n",
    "temp = model_gray.predict(x_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing prediction shape and scores\n",
    "print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "print()\n",
    "\n",
    "    \n",
    "# Getting indexes of maximum values along specified axis\n",
    "temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "# Check point\n",
    "# Showing prediction shape after convertion\n",
    "# Showing predicted and correct indexes of classes\n",
    "print('prediction shape  :', temp.shape)  # (10000,)\n",
    "print('predicted indexes :', temp[0:10])\n",
    "print('correct indexes   :', y_test[:10])\n",
    "print()\n",
    "\n",
    "\n",
    "# Calculating accuracy\n",
    "# We compare predicted class with correct class for all input images\n",
    "# By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "# By function 'np.mean' we calculate mean value:\n",
    "# all_True / (all_True + all_False)\n",
    "accuracy = np.mean(temp == y_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing calculated accuracy\n",
    "print('Testing accuracy  : {0:.5f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 5: Visualizing filters of convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the weights from entire model\n",
    "w = model_gray.get_weights()\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n",
    "\n",
    "\n",
    "# Moving last dimension to the first position\n",
    "# Reshaping Numpy array from\n",
    "# (height, width, channels, number) to\n",
    "# (number, height, width, channels)\n",
    "w[0] = w[0].transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Converting weights of convolutional layer into a grid\n",
    "grid = convert_to_grid(w[0])\n",
    "\n",
    "\n",
    "# Plotting the grid\n",
    "plt.imshow(grid.astype('uint8'), cmap='gray')\n",
    "\n",
    "\n",
    "# Switching off axes\n",
    "# Setting size of the plot (width, height) in inches\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST, 1st GRAY model\n",
    "### GRAY dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 4: Converting intermediate weights into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterating all saved intermediate weights\n",
    "for i in range(550):\n",
    "    # Opening saved binary file with intermediate weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File('mnist/intermediate/' + '{0:04d}'.format(i) + \\\n",
    "                   '_w_1_mnist_gray_255_mean.h5', 'r') as f:\n",
    "        # Extracting saved array with intermediate weights by appropriate key\n",
    "        # Saving them into new variable\n",
    "        weights = f['weights_layer_0']  # HDF5 dataset\n",
    "\n",
    "        # Converting them into Numpy array\n",
    "        weights = np.array(weights)  # Numpy arrays\n",
    "\n",
    "    \n",
    "    # Moving last dimension to the first position\n",
    "    # Reshaping Numpy array from\n",
    "    # (height, width, channels, number) to\n",
    "    # (number, height, width, channels)\n",
    "    weights = weights.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    \n",
    "    # Converting weights of convolutional layer into a grid\n",
    "    grid = convert_to_grid(weights)\n",
    "    \n",
    "    \n",
    "    # Plotting the grid\n",
    "    plt.imshow(grid.astype('uint8'), cmap='gray')\n",
    "    \n",
    "    \n",
    "    # Switching off axes\n",
    "    # Setting size of the plot (width, height) in inches\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    \n",
    "    \n",
    "    # Saving the plot into image file\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    plt.savefig('mnist/intermediate/images/' + '{0:04d}'.format(i) + '.png', dpi=300)\n",
    "    \n",
    "    \n",
    "    # Closing the plot\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('{0:04d}.png is created'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + \n",
    "               'ts' + '/' + \n",
    "               'dataset_ts_rgb_255_mean.hdf5', 'r') as f:\n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 43)\n",
    "y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 2: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb = load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_1_ts_rgb.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning new spatial dimension for the filters of the 1st convolutional layer\n",
    "model_rgb.layers[0].kernel_size = (24, 24)\n",
    "\n",
    "\n",
    "# Re-loading model and initializing it by new structure\n",
    "model_rgb = model_from_json(model_rgb.to_json())\n",
    "\n",
    "\n",
    "# Re-compiling model\n",
    "model_rgb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully re-loaded')\n",
    "print('Model is successfully re-compiled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing spatial size of filters for the 1st convolutional layer\n",
    "print(model_rgb.layers[0].kernel_size)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shapes of the weights for the 1st convolutional layer\n",
    "w = model_rgb.get_weights()\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing filepath to save best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "best_weights_filepath = 'ts' + '/' + 'w_1_ts_rgb_255_mean.h5'\n",
    "\n",
    "\n",
    "# Defining schedule to save best weights\n",
    "best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                               save_weights_only=True,                                   \n",
    "                               monitor='val_accuracy',\n",
    "                               mode='max',\n",
    "                               save_best_only=True,\n",
    "                               period=1,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save best weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining schedule to save intermediate weights\n",
    "class CustomCallback(Callback):\n",
    "    # Constructor of the class\n",
    "    def __init__(self):\n",
    "        # Defining variable to be as a part of filename\n",
    "        self.filename = 0\n",
    "        \n",
    "    # Function that is called at the end of every batch\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Checking if it is every 100th batch\n",
    "        if batch % 100 == 0:\n",
    "            # Preparing filepath to save intermediate weights\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            intermediate_weights_filepath = 'ts' + '/' + \\\n",
    "                                            'intermediate' + '/' + \\\n",
    "                                            '{0:04d}'.format(self.filename) + \\\n",
    "                                            '_w_1_ts_rgb_255_mean.h5'\n",
    "            \n",
    "            # Getting weights only for the first convolutional layer\n",
    "            weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "            # Saving obtained weights into new HDF5 binary file\n",
    "            # Initiating File object\n",
    "            # Creating file with current name\n",
    "            # Opening it in writing mode by 'w'\n",
    "            # (!) On Windows, it might need to change\n",
    "            # this: + '/' +\n",
    "            # to this: + '\\' +\n",
    "            # or to this: + '\\\\' +\n",
    "            with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                # Calling method to create dataset of given shape and type\n",
    "                # Saving Numpy array with weights from the first layer\n",
    "                f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "            # Increasing variable to be as a part of the next filename\n",
    "            self.filename += 1\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Schedule to save intermediate weights is created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model_rgb.fit(x_train, y_train,\n",
    "                  batch_size=50,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_validation, y_validation),\n",
    "                  callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                  verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 4: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and assigning best weights\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model_rgb.load_weights('ts' + '/' + 'w_1_ts_rgb_255_mean.h5')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Best weights for 1st RGB model are loaded and assigned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RGB model\n",
    "temp = model_rgb.predict(x_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing prediction shape and scores\n",
    "print('prediction shape  :', temp.shape)  # (3111, 43)\n",
    "print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "print()\n",
    "\n",
    "    \n",
    "# Getting indexes of maximum values along specified axis\n",
    "temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "# Check point\n",
    "# Showing prediction shape after convertion\n",
    "# Showing predicted and correct indexes of classes\n",
    "print('prediction shape  :', temp.shape)  # (3111,)\n",
    "print('predicted indexes :', temp[0:10])\n",
    "print('correct indexes   :', y_test[:10])\n",
    "print()\n",
    "\n",
    "\n",
    "# Calculating accuracy\n",
    "# We compare predicted class with correct class for all input images\n",
    "# By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "# By function 'np.mean' we calculate mean value:\n",
    "# all_True / (all_True + all_False)\n",
    "accuracy = np.mean(temp == y_test)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing calculated accuracy\n",
    "print('Testing accuracy  : {0:.5f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 5: Visualizing filters of convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the weights from entire model\n",
    "w = model_rgb.get_weights()\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n",
    "\n",
    "\n",
    "# Moving last dimension to the first position\n",
    "# Reshaping Numpy array from\n",
    "# (height, width, channels, number) to\n",
    "# (number, height, width, channels)\n",
    "w[0] = w[0].transpose(3, 0, 1, 2)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing shape of the weights for convolutional layer\n",
    "print(w[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Converting weights of convolutional layer into a grid\n",
    "grid = convert_to_grid(w[0])\n",
    "\n",
    "\n",
    "# Plotting the grid\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "\n",
    "\n",
    "# Switching off axes\n",
    "# Setting size of the plot (width, height) in inches\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs, 1st RGB model\n",
    "### RGB dataset (255.0 ==> mean)\n",
    "\n",
    "## Step 6: Converting intermediate weights into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterating all saved intermediate weights\n",
    "for i in range(400):\n",
    "    # Opening saved binary file with intermediate weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File('ts/intermediate/' + '{0:04d}'.format(i) + \\\n",
    "                   '_w_1_ts_rgb_255_mean.h5', 'r') as f:\n",
    "        # Extracting saved array with intermediate weights by appropriate key\n",
    "        # Saving them into new variable\n",
    "        weights = f['weights_layer_0']  # HDF5 dataset\n",
    "\n",
    "        # Converting them into Numpy array\n",
    "        weights = np.array(weights)  # Numpy arrays\n",
    "\n",
    "    \n",
    "    # Moving last dimension to the first position\n",
    "    # Reshaping Numpy array from\n",
    "    # (height, width, channels, number) to\n",
    "    # (number, height, width, channels)\n",
    "    weights = weights.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    \n",
    "    # Converting weights of convolutional layer into a grid\n",
    "    grid = convert_to_grid(weights)\n",
    "    \n",
    "    \n",
    "    # Plotting the grid\n",
    "    plt.imshow(grid.astype('uint8'))\n",
    "    \n",
    "    \n",
    "    # Switching off axes\n",
    "    # Setting size of the plot (width, height) in inches\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    \n",
    "    \n",
    "    # Saving the plot into image file\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    plt.savefig('ts/intermediate/images/' + '{0:04d}'.format(i) + '.png', dpi=300)\n",
    "    \n",
    "    \n",
    "    # Closing the plot\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('{0:04d}.png is created'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'model_from_json':  \n",
    "**print(help(model_from_json))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://keras.io/api/models/model_saving_apis/#modelfromjson-function  \n",
    "  \n",
    "  \n",
    "To get more details for usage of 'to_json':  \n",
    "**print(help(Sequential().to_json))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://keras.io/api/models/model_saving_apis/#tojson-method  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(model_from_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "print(help(Sequential().to_json))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
