{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-5\n",
    "### Construct deep architectures for CNN models\n",
    "#### How much Dropout?\n",
    "\n",
    "**Description:**  \n",
    "*Analyze percentage of dropout after every layer  \n",
    "Interpret notation*\n",
    "\n",
    "**File:** *dropout.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Open preprocessed dataset  \n",
    "**--> Step 2:** Convert classes vectors to binary matrices  \n",
    "**--> Step 3:** Choose **percentage of dropout**  \n",
    "**--> Step 4:** Visualize built CNN models  \n",
    "**--> Step 5:** Set up learning rate & epochs  \n",
    "**--> Step 6:** Train built CNN models  \n",
    "**--> Step 7:** Show and plot accuracies  \n",
    "**--> Step 8:** Make a conclusion  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Chosen architecture for every preprocessed dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full path to preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved custom dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + \n",
    "               'dataset_custom_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[3])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 5)\n",
    "y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Choosing percentage of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "**C** - convolutional layer  \n",
    "**P** - pooling  \n",
    "**D** - dropout  \n",
    "  \n",
    "Examples:\n",
    "* **8C5** - convolutional layer with 8 feature maps and kernels of spatial size 5x5  \n",
    "* **P2** - pooling operation with 2x2 window and stride 2  \n",
    "*  **128** - fully connected layer (dense layer) with 128 neurons  \n",
    "*  **D15** - 15% of dropout  \n",
    "  \n",
    "Definitions:\n",
    "* **filters** (also called as kernels or cores) are trainable parameters  \n",
    "* **weights** are values of filters that network learns during training  \n",
    "* **strides** are steps by which window of filter size goes through the input  \n",
    "* **padding** is a 0-valued frame used to process edges of the input  \n",
    "* **Dropout** is a regularization technique that helps to prevent overfitting  \n",
    "  \n",
    "Some keywords values:\n",
    "* **kernel_size=5** sets the filter size to be 5x5\n",
    "* **strides=1** is a default value\n",
    "* **padding='valid'** is a default value, meaning that output will be reduced: kernel_size - 1  \n",
    "* **padding='same'** means that output will be of the same spatial size as input  \n",
    "* **activation='relu'** sets ReLU (Rectified Linear Unit) as activation function  \n",
    "  \n",
    "Calculations of spatial size for feature maps after convolutional layer:  \n",
    "* **height_output = 1 + (height_input + 2 * pad - kernel_size) / stride**\n",
    "* **width_output = 1 + (width_input + 2 * pad - kernel_size) / stride**\n",
    "  \n",
    "Example without pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "* **width_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "  \n",
    "Example with pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "* **width_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Building 5 models\n",
    "# RGB --> {64C5-P2} --> {128C5-P2} --> {256C5-P2} --> {512C5-P2} --> 2048 --> 5\n",
    "# RGB --> {64C5-P2-D10} --> {128C5-P2-D10} --> {256C5-P2-D10} --> {512C5-P2-D10} --> 2048-D10 --> 5\n",
    "# RGB --> {64C5-P2-D20} --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> 2048-D20 --> 5\n",
    "# RGB --> {64C5-P2-D30} --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> 2048-D30 --> 5\n",
    "# RGB --> {64C5-P2-D40} --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> 2048-D40 --> 5\n",
    "\n",
    "\n",
    "# Defining list to collect models in\n",
    "model = []\n",
    "\n",
    "\n",
    "# Building models in a loop\n",
    "for i in range(5):\n",
    "    # Initializing model to be as linear stack of layers\n",
    "    temp = Sequential()\n",
    "\n",
    "    # Adding first convolutional-pooling pair\n",
    "    temp.add(Conv2D(64, kernel_size=5, padding='same', activation='relu', input_shape=(64, 64, 3)))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding second convolutional-pooling pair\n",
    "    temp.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding third convolutional-pooling pair\n",
    "    temp.add(Conv2D(256, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fourth convolutional-pooling pair\n",
    "    temp.add(Conv2D(512, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fully connected layers\n",
    "    temp.add(Flatten())\n",
    "    temp.add(Dense(2048, activation='relu'))\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "    temp.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Compiling created model\n",
    "    temp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Adding current model in the list\n",
    "    model.append(temp)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('5 models are compiled successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Visualizing built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting model's layers in form of flowchart\n",
    "plot_model(model[4],\n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='TB',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Showing model's summary in form of table\n",
    "model[4].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing dropout rate\n",
    "model[4].layers[2].rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Training built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training models in a loop\n",
    "for i in range(5):\n",
    "    # Current model\n",
    "    temp = model[i].fit(x_train, y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation, y_validation),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Adding results of current model in the list\n",
    "    h.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 7: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the models\n",
    "for i in range(5):\n",
    "    print('Model {0}: Training accuracy={1:.5f}, Validation accuracy={2:.5f}'.\n",
    "                                                         format(i + 1,\n",
    "                                                                max(h[i].history['accuracy']),\n",
    "                                                                max(h[i].history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "plt.plot(h[4].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.62, 0.72)\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['model_1', 'model_2', 'model_3', 'model_4', 'model_5'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Models accuracies: Custom Dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 8: Making a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to validation accuracy, the 3rd model has the highest value\n",
    "\n",
    "# The choice for custom dataset is 3rd model\n",
    "# RGB input --> {64C5-P2-D20} --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> 2048-D20 --> 5\n",
    "# GRAY input --> {64C5-P2-D20} --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> 2048-D20 --> 5\n",
    "\n",
    "# RGB input: (64, 64, 3)\n",
    "# GRAY input: (64, 64, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + \n",
    "               'dataset_cifar10_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[3])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Choosing percentage of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "**C** - convolutional layer  \n",
    "**P** - pooling  \n",
    "**D** - dropout  \n",
    "  \n",
    "Examples:\n",
    "* **8C5** - convolutional layer with 8 feature maps and kernels of spatial size 5x5  \n",
    "* **P2** - pooling operation with 2x2 window and stride 2  \n",
    "*  **128** - fully connected layer (dense layer) with 128 neurons  \n",
    "*  **D15** - 15% of dropout  \n",
    "  \n",
    "Definitions:\n",
    "* **filters** (also called as kernels or cores) are trainable parameters  \n",
    "* **weights** are values of filters that network learns during training  \n",
    "* **strides** are steps by which window of filter size goes through the input  \n",
    "* **padding** is a 0-valued frame used to process edges of the input  \n",
    "* **Dropout** is a regularization technique that helps to prevent overfitting  \n",
    "  \n",
    "Some keywords values:\n",
    "* **kernel_size=5** sets the filter size to be 5x5\n",
    "* **strides=1** is a default value\n",
    "* **padding='valid'** is a default value, meaning that output will be reduced: kernel_size - 1  \n",
    "* **padding='same'** means that output will be of the same spatial size as input  \n",
    "* **activation='relu'** sets ReLU (Rectified Linear Unit) as activation function  \n",
    "  \n",
    "Calculations of spatial size for feature maps after convolutional layer:  \n",
    "* **height_output = 1 + (height_input + 2 * pad - kernel_size) / stride**\n",
    "* **width_output = 1 + (width_input + 2 * pad - kernel_size) / stride**\n",
    "  \n",
    "Example without pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "* **width_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "  \n",
    "Example with pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "* **width_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 5 models\n",
    "# RGB --> {128C5-P2} --> {256C5-P2} --> {512C5-P2} --> 256 --> 10\n",
    "# RGB --> {128C5-P2-D10} --> {256C5-P2-D10} --> {512C5-P2-D10} --> 256-D10 --> 10\n",
    "# RGB --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> 256-D20 --> 10\n",
    "# RGB --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> 256-D30 --> 10\n",
    "# RGB --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> 256-D40 --> 10\n",
    "\n",
    "\n",
    "# Defining list to collect models in\n",
    "model = []\n",
    "\n",
    "\n",
    "# Building models in a loop\n",
    "for i in range(5):\n",
    "    # Initializing model to be as linear stack of layers\n",
    "    temp = Sequential()\n",
    "\n",
    "    # Adding first convolutional-pooling pair\n",
    "    temp.add(Conv2D(128, kernel_size=5, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding second convolutional-pooling pair\n",
    "    temp.add(Conv2D(256, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding third convolutional-pooling pair\n",
    "    temp.add(Conv2D(512, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fully connected layers\n",
    "    temp.add(Flatten())\n",
    "    temp.add(Dense(256, activation='relu'))\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "    temp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compiling created model\n",
    "    temp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Adding current model in the list\n",
    "    model.append(temp)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('5 models are compiled successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Visualizing built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting model's layers in form of flowchart\n",
    "plot_model(model[4],\n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='TB',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Showing model's summary in form of table\n",
    "model[4].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing dropout rate\n",
    "model[4].layers[2].rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Training built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training models in a loop\n",
    "for i in range(5):\n",
    "    # Сurrent model\n",
    "    temp = model[i].fit(x_train, y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation, y_validation),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Adding results of current model in the list\n",
    "    h.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 7: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the models\n",
    "for i in range(5):\n",
    "    print('Model {0}: Training accuracy={1:.5f}, Validation accuracy={2:.5f}'.\n",
    "                                                         format(i + 1,\n",
    "                                                                max(h[i].history['accuracy']),\n",
    "                                                                max(h[i].history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "plt.plot(h[4].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.69, 0.832)\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['model_1', 'model_2', 'model_3', 'model_4', 'model_5'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Models accuracies: CIFAR-10 dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 8: Making a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to validation accuracy, the 4th and 5th models have the highest values\n",
    "\n",
    "# The choice for CIFAR-10 dataset is 5th model\n",
    "# RGB input --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> 256-D40 --> 10\n",
    "# GRAY input --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> 256-D40 --> 10\n",
    "\n",
    "# RGB input: (32, 32, 3)\n",
    "# GRAY input: (32, 32, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved MNIST dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + \n",
    "               'dataset_mnist_gray_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[3])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Choosing percentage of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "**C** - convolutional layer  \n",
    "**P** - pooling  \n",
    "**D** - dropout  \n",
    "  \n",
    "Examples:\n",
    "* **8C5** - convolutional layer with 8 feature maps and kernels of spatial size 5x5  \n",
    "* **P2** - pooling operation with 2x2 window and stride 2  \n",
    "*  **128** - fully connected layer (dense layer) with 128 neurons  \n",
    "*  **D15** - 15% of dropout  \n",
    "  \n",
    "Definitions:\n",
    "* **filters** (also called as kernels or cores) are trainable parameters  \n",
    "* **weights** are values of filters that network learns during training  \n",
    "* **strides** are steps by which window of filter size goes through the input  \n",
    "* **padding** is a 0-valued frame used to process edges of the input  \n",
    "* **Dropout** is a regularization technique that helps to prevent overfitting  \n",
    "  \n",
    "Some keywords values:\n",
    "* **kernel_size=5** sets the filter size to be 5x5\n",
    "* **strides=1** is a default value\n",
    "* **padding='valid'** is a default value, meaning that output will be reduced: kernel_size - 1  \n",
    "* **padding='same'** means that output will be of the same spatial size as input  \n",
    "* **activation='relu'** sets ReLU (Rectified Linear Unit) as activation function  \n",
    "  \n",
    "Calculations of spatial size for feature maps after convolutional layer:  \n",
    "* **height_output = 1 + (height_input + 2 * pad - kernel_size) / stride**\n",
    "* **width_output = 1 + (width_input + 2 * pad - kernel_size) / stride**\n",
    "  \n",
    "Example without pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "* **width_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "  \n",
    "Example with pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "* **width_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 5 models\n",
    "# GRAY --> {128C5-P2} --> {256C5-P2} --> {512C5-P2} --> 256 --> 10\n",
    "# GRAY --> {128C5-P2-D10} --> {256C5-P2-D10} --> {512C5-P2-D10} --> 256-D10 --> 10\n",
    "# GRAY --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> 256-D20 --> 10\n",
    "# GRAY --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> 256-D30 --> 10\n",
    "# GRAY --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> 256-D40 --> 10\n",
    "\n",
    "\n",
    "# Defining list to collect models in\n",
    "model = []\n",
    "\n",
    "\n",
    "# Building models in a loop\n",
    "for i in range(5):\n",
    "    # Initializing model to be as linear stack of layers\n",
    "    temp = Sequential()\n",
    "\n",
    "    # Adding first convolutional-pooling pair\n",
    "    temp.add(Conv2D(128, kernel_size=5, padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding second convolutional-pooling pair\n",
    "    temp.add(Conv2D(256, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding third convolutional-pooling pair\n",
    "    temp.add(Conv2D(512, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fully connected layers\n",
    "    temp.add(Flatten())\n",
    "    temp.add(Dense(256, activation='relu'))\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "    temp.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compiling created model\n",
    "    temp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Adding current model in the list\n",
    "    model.append(temp)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('5 models are compiled successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Visualizing built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting model's layers in form of flowchart\n",
    "plot_model(model[4],\n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='TB',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's summary in form of table\n",
    "model[4].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing dropout rate\n",
    "model[1].layers[2].rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Training built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training models in a loop\n",
    "for i in range(5):\n",
    "    # Сurrent model\n",
    "    temp = model[i].fit(x_train, y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation, y_validation),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Adding results of current model in the list\n",
    "    h.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 7: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the models\n",
    "for i in range(5):\n",
    "    print('Model {0}: Training accuracy={1:.5f}, Validation accuracy={2:.5f}'.\n",
    "                                                         format(i + 1,\n",
    "                                                                max(h[i].history['accuracy']),\n",
    "                                                                max(h[i].history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "plt.plot(h[4].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.984, 0.9951)\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['model_1', 'model_2', 'model_3', 'model_4', 'model_5'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Models accuracies: MNIST dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 8: Making a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to validation accuracy, the 4th and 5th models have the highest values\n",
    "\n",
    "# The choice for MNIST dataset is 4th model\n",
    "# GRAY input --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> 256-D30 --> 10\n",
    "\n",
    "# GRAY input: (28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + \n",
    "               'dataset_ts_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[3])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 43)\n",
    "y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Choosing percentage of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "**C** - convolutional layer  \n",
    "**P** - pooling  \n",
    "**D** - dropout  \n",
    "  \n",
    "Examples:\n",
    "* **8C5** - convolutional layer with 8 feature maps and kernels of spatial size 5x5  \n",
    "* **P2** - pooling operation with 2x2 window and stride 2  \n",
    "*  **128** - fully connected layer (dense layer) with 128 neurons  \n",
    "*  **D15** - 15% of dropout  \n",
    "  \n",
    "Definitions:\n",
    "* **filters** (also called as kernels or cores) are trainable parameters  \n",
    "* **weights** are values of filters that network learns during training  \n",
    "* **strides** are steps by which window of filter size goes through the input  \n",
    "* **padding** is a 0-valued frame used to process edges of the input  \n",
    "* **Dropout** is a regularization technique that helps to prevent overfitting  \n",
    "  \n",
    "Some keywords values:\n",
    "* **kernel_size=5** sets the filter size to be 5x5\n",
    "* **strides=1** is a default value\n",
    "* **padding='valid'** is a default value, meaning that output will be reduced: kernel_size - 1  \n",
    "* **padding='same'** means that output will be of the same spatial size as input  \n",
    "* **activation='relu'** sets ReLU (Rectified Linear Unit) as activation function  \n",
    "  \n",
    "Calculations of spatial size for feature maps after convolutional layer:  \n",
    "* **height_output = 1 + (height_input + 2 * pad - kernel_size) / stride**\n",
    "* **width_output = 1 + (width_input + 2 * pad - kernel_size) / stride**\n",
    "  \n",
    "Example without pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "* **width_output = 1 + (64 + 2 * 0 - 5) / 1 = 60**\n",
    "  \n",
    "Example with pad frame:\n",
    "* **height_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "* **width_output = 1 + (64 + 2 * 2 - 5) / 1 = 64**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 5 models\n",
    "# RGB --> {128C5-P2} --> {256C5-P2} --> {512C5-P2} --> {1024C3-P2} --> 2048 --> 43\n",
    "# RGB --> {128C5-P2-D10} --> {256C5-P2-D10} --> {512C5-P2-D10} --> {1024C3-P2-D10} --> 2048-D10 --> 43\n",
    "# RGB --> {128C5-P2-D20} --> {256C5-P2-D20} --> {512C5-P2-D20} --> {1024C3-P2-D20} --> 2048-D20 --> 43\n",
    "# RGB --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> {1024C3-P2-D30} --> 2048-D30 --> 43\n",
    "# RGB --> {128C5-P2-D40} --> {256C5-P2-D40} --> {512C5-P2-D40} --> {1024C3-P2-D40} --> 2048-D40 --> 43\n",
    "\n",
    "\n",
    "# Defining list to collect models in\n",
    "model = []\n",
    "\n",
    "\n",
    "# Building models in a loop\n",
    "for i in range(5):\n",
    "    # Initializing model to be as linear stack of layers\n",
    "    temp = Sequential()\n",
    "\n",
    "    # Adding first convolutional-pooling pair\n",
    "    temp.add(Conv2D(128, kernel_size=5, padding='same', activation='relu', input_shape=(48, 48, 3)))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding second convolutional-pooling pair\n",
    "    temp.add(Conv2D(256, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding third convolutional-pooling pair\n",
    "    temp.add(Conv2D(512, kernel_size=5, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fourth convolutional-pooling pair\n",
    "    temp.add(Conv2D(1024, kernel_size=3, padding='same', activation='relu'))\n",
    "    temp.add(MaxPool2D())\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "\n",
    "    # Adding fully connected layers\n",
    "    temp.add(Flatten())\n",
    "    temp.add(Dense(2048, activation='relu'))\n",
    "    temp.add(Dropout(0.1 * i))\n",
    "    temp.add(Dense(43, activation='softmax'))\n",
    "\n",
    "    # Compiling created model\n",
    "    temp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Adding current model in the list\n",
    "    model.append(temp)\n",
    "    \n",
    "\n",
    "# Check point\n",
    "print('5 models are compiled successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Visualizing built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting model's layers in form of flowchart\n",
    "plot_model(model[4],\n",
    "           to_file='model.png',\n",
    "           show_shapes=True,\n",
    "           show_layer_names=False,\n",
    "           rankdir='TB',\n",
    "           dpi=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's summary in form of table\n",
    "model[4].summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing dropout rate\n",
    "model[3].layers[2].rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Training built CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training models in a loop\n",
    "for i in range(5):\n",
    "    # Сurrent model\n",
    "    temp = model[i].fit(x_train, y_train,\n",
    "                        batch_size=50,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation, y_validation),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Adding results of current model in the list\n",
    "    h.append(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 7: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the models\n",
    "for i in range(5):\n",
    "    print('Model {0}: Training accuracy={1:.5f}, Validation accuracy={2:.5f}'.\n",
    "                                                          format(i + 1,\n",
    "                                                                 max(h[i].history['accuracy']),\n",
    "                                                                 max(h[i].history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "plt.plot(h[4].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.9915, 0.9986)\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['model_1', 'model_2', 'model_3', 'model_4', 'model_5'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Models accuracies: Traffic Signs dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 8: Making a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to validation accuracy, the 4th and 5th models have the highest values\n",
    "\n",
    "# The choice for Traffic Signs dataset is 4th model\n",
    "# RGB input --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> {1024C3-P2-D30} --> 2048-D30 --> 43\n",
    "# GRAY input --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> {1024C3-P2-D30} --> 2048-D30 --> 43\n",
    "\n",
    "# RGB input: (48, 48, 3)\n",
    "# GRAY input: (48, 48, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'Dropout' class:  \n",
    "**print(help(Dropout))**  \n",
    "  \n",
    "More details and examples are here:  \n",
    "https://keras.io/api/layers/regularization_layers/dropout/\n",
    "\n",
    "\n",
    "To get more details for usage of 'Sequential' class:  \n",
    "**print(help(Sequential))**  \n",
    "  \n",
    "More details and examples are here:  \n",
    "https://keras.io/api/models/sequential/\n",
    "\n",
    "\n",
    "To get more details for usage of function 'to_categorical':  \n",
    "**print(help(to_categorical))**  \n",
    "\n",
    "More details and examples are here:  \n",
    "https://keras.io/api/utils/python_utils/#to_categorical-function \n",
    "\n",
    "\n",
    "To get more details for usage of function 'plot_model':  \n",
    "**print(help(plot_model))**  \n",
    "\n",
    "More details and examples are here:  \n",
    "https://keras.io/api/utils/model_plotting_utils/#plot_model-function  \n",
    "\n",
    "\n",
    "To get more details for usage of function 'plt.plot':  \n",
    "**print(help(plt.plot))**  \n",
    "\n",
    "More details and examples are here:  \n",
    "https://matplotlib.org/3.1.3/api/_as_gen/matplotlib.pyplot.plot.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(Dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(Sequential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(help(to_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(plot_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(plt.plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Defining array with random values\n",
    "x_train = np.random.randint(5, size=(5, 2)).astype(np.float32)\n",
    "\n",
    "\n",
    "# Showing array\n",
    "print(x_train)\n",
    "print()\n",
    "\n",
    "\n",
    "# Initializing Dropout layer\n",
    "layer = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "# Passing array to the layer\n",
    "output = layer(x_train, training=True)\n",
    "\n",
    "\n",
    "# Showing array after Dropout layer\n",
    "print(output)\n",
    "print()\n",
    "\n",
    "# Showing initial array\n",
    "print(x_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
