{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## &nbsp; â›©ï¸ Section-9\n",
    "### &nbsp; &nbsp; ðŸŽ›ï¸ What does Confusion Matrix show?\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**Description:**  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Explained theory on what Confusion Matrix shows*  \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**File:** *confusion_matrix.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Algorithm:\n",
    "\n",
    "**--> Step 1:** Generate 2 Vectors: True & Predicted  \n",
    "**--> Step 2: Display Confusion Matrix**  \n",
    "**--> Step 3:** Build Classification Report  \n",
    "\n",
    "\n",
    "âœ”ï¸ **Result:**  \n",
    "- Confusion Matrix  \n",
    "- Classification report  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¾ Preparing classes' labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining list with labels\n",
    "labels = ['Horse', 'Tiger', 'Cat', 'Dog', 'Polar bear']\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing labels\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ° Generating True Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating Numpy array with True classes' numbers\n",
    "y_true = np.random.randint(low=0, high=len(labels), size=100, dtype=int)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Shwoing array\n",
    "print(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Showing distribution of samples among classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of samples for every class\n",
    "# Iterating all classes in 'y_true' array\n",
    "# Using Numpy function 'unique'\n",
    "# Returning sorted unique elements and their frequencies\n",
    "classesIndexes, classesFrequency = np.unique(y_true, return_counts=True)\n",
    "\n",
    "\n",
    "# Printing frequency (number of samples) for every class\n",
    "print('classes indexes:' , classesIndexes)\n",
    "print('\\n')\n",
    "print('classes frequency:', classesFrequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "\n",
    "\n",
    "# Plotting histogram of 5 classes with their number of samples\n",
    "# Defining a figure object \n",
    "figure = plt.figure()\n",
    "\n",
    "\n",
    "# Plotting Bar chart\n",
    "plt.bar(classesIndexes, classesFrequency, align='center', alpha=0.6)\n",
    "\n",
    "\n",
    "# Giving name to X & Y axes\n",
    "plt.xlabel('\\nClass name', fontsize=18)\n",
    "plt.ylabel('Class frequency\\n', fontsize=18)\n",
    "\n",
    "\n",
    "# Giving names to every Bar along X axis\n",
    "plt.xticks(classesIndexes, labels, fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Histogram', fontsize=22)\n",
    "\n",
    "\n",
    "# Saving the plot\n",
    "figure.savefig('histogram.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ° Generating Predicted Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making copy of array with True classes' numbers\n",
    "y_predicted = np.copy(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing randomly 25% of classes to be changed\n",
    "ii = np.random.randint(low=0, high=len(y_predicted), size=int(0.25 * len(y_predicted)), dtype=int)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing chosen indexes\n",
    "print(ii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating chosen indexes and replacing them with other classes' numbers\n",
    "for i in ii:\n",
    "    # Generating new class index\n",
    "    y_predicted[i] = np.random.randint(low=0, high=len(labels), dtype=int)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing difference between True classes' numbers and Predicted ones\n",
    "    print('index = {0:2d}, True class => {1}, {2} <= Predicted class'.\n",
    "          format(i, y_true[i], y_predicted[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§® Calculating Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual, True class\n",
    "# Each column represents a number of predicted class\n",
    "\n",
    "\n",
    "# Computing Confusion Matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_true, y_predicted)\n",
    "\n",
    "# Showing Confusion Matrix in form of 2D Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‘ï¸â€ðŸ—¨ï¸ Displaying Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of Confusion Matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels)\n",
    "\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
    "# Other possible options for colour map are:\n",
    "# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix', fontsize=24)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš–ï¸ Building Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics\n",
    "print(classification_report(y_true, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—’ï¸ Some comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more details for usage of *'np.copy':*  \n",
    "**print(help(np.copy))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://numpy.org/doc/stable/reference/generated/numpy.copy.html  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "To get more details for usage of *'confusion_matrix':*  \n",
    "**print(help(confusion_matrix))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://www.sklearn.org/modules/generated/sklearn.metrics.confusion_matrix.html  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "To get more details for usage of *'classification_report':*  \n",
    "**print(help(classification_report))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html  \n",
    "  \n",
    "  <br/>\n",
    "  \n",
    "To get more details for usage of *'plt.colormaps()':*  \n",
    "**print(help(plt.colormaps()))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " -  https://matplotlib.org/api/pyplot_summary.html?highlight=colormaps#matplotlib.pyplot.colormaps  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(np.copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(classification_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(plt.colormaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(plt.rcParams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(plt.savefig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(plt.xticks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
