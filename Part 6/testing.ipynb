{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-6\n",
    "### Test trained CNNs models in Keras\n",
    "\n",
    "**Description:**  \n",
    "*Evaluate accuracy of every deep model on testing dataset  \n",
    "Display confusion matrix*  \n",
    "\n",
    "**File:** *testing.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Load saved CNN model  \n",
    "**--> Step 2:** Load and assign best weights  \n",
    "**--> Step 3: Predict with test dataset**  \n",
    "**--> Step 4:** Build classification report & confusion matrix  \n",
    "**--> Step 5:** Test on one image  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Accuracy results  \n",
    "- Classification reports  \n",
    "- Confusion matrices  \n",
    "- Classified one image  \n",
    "- Bar chart of classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section2' with labels for CIFAR-10 dataset\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section2'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section2'\n",
    "full_path_to_Section2 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section2'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section3' with labels for Traffic Signs dataset\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section3'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section3'\n",
    "full_path_to_Section3 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section3'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section4'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section5' with designed models\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section5'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section5'\n",
    "full_path_to_Section5 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_1_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_1_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_1_custom_rgb_255_mean.h5',\n",
    "           'w_1_custom_rgb_255_mean_std.h5',\n",
    "           'w_1_custom_gray_255_mean.h5',\n",
    "           'w_1_custom_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 1st model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('custom' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('custom' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 1st model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (278, 5)\n",
    "            print('prediction scores :', temp[0])  # 5 score numbers\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (278,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "        \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class\n",
    "# Each column represents a number of predicted class\n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing labels for custom dataset\n",
    "labels_custom = ['Horse', 'Tiger', 'Cat', 'Dog', 'Polar bear']\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing labels\n",
    "print(labels_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels_custom)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
    "# Other possible options for colour map are:\n",
    "# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 1st model, Custom Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_1_custom_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Mean Image for RGB custom dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + \n",
    "               '/' + 'mean_rgb_dataset_custom.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_rgb = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_rgb = np.array(mean_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for RGB custom dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + \n",
    "               'std_rgb_dataset_custom.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_rgb = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_rgb = np.array(std_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Mean Image for GRAY custom dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + \n",
    "               'mean_gray_dataset_custom.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_gray = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_gray = np.array(mean_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for GRAY custom dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + \n",
    "               'std_gray_dataset_custom.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_gray = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_gray = np.array(std_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_custom_bgr = cv2.imread('images_to_test' + '/' + 'custom_to_test_1.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_custom_rgb = cv2.cvtColor(image_custom_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 64 by 64 pixels size\n",
    "image_custom_rgb = cv2.resize(image_custom_rgb,\n",
    "                              (64, 64),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_custom_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_custom_rgb_255 = image_custom_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_custom_rgb_255_mean = image_custom_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_custom_rgb_255_mean_std = image_custom_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_custom_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_custom_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_custom_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_custom_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_custom_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_custom_gray = cv2.cvtColor(image_custom_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_custom_gray = image_custom_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_custom_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_custom_gray_255 = image_custom_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_custom_gray_255_mean = image_custom_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_custom_gray_255_mean_std = image_custom_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_custom_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_custom_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_custom_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_custom_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_custom_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_custom_rgb_255_mean = image_custom_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_custom_rgb_255_mean_std = image_custom_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_custom_gray_255_mean = image_custom_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_custom_gray_255_mean_std = image_custom_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_custom_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_custom_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_custom_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_custom_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to plot bar chart with scores values\n",
    "def bar_chart(scores, bar_title, show_xticks=True, labels=None):\n",
    "    # Arranging X axis\n",
    "    x_positions = np.arange(scores.size)\n",
    "\n",
    "    # Creating bar chart\n",
    "    barlist = plt.bar(x_positions, scores, align='center', alpha=0.6)\n",
    "\n",
    "    # Highlighting the highest bar\n",
    "    barlist[np.argmax(scores)].set_color('red')\n",
    "\n",
    "    # Giving labels to bars along X axis\n",
    "    if show_xticks:\n",
    "        plt.xticks(x_positions, labels, rotation=20, fontsize=15)\n",
    "\n",
    "    # Giving name to axes\n",
    "    plt.xlabel('Class', fontsize=20)\n",
    "    plt.ylabel('Value', fontsize=20)\n",
    "\n",
    "    # Giving name to bar chart\n",
    "    plt.title('Classification: ' + bar_title, fontsize=20)\n",
    "\n",
    "    # Showing bar chart\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Function to plot Bar Chart is successfully defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_custom_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_custom_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],          \n",
    "          bar_title='1st RGB model, custom_rgb_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_custom_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_custom_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st RGB model, custom_rgb_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_custom_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_custom_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],          \n",
    "          bar_title='1st GRAY model, custom_gray_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_custom_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_custom_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],          \n",
    "          bar_title='1st GRAY model, custom_gray_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_2_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_2_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_2_custom_rgb_255_mean.h5',\n",
    "           'w_2_custom_rgb_255_mean_std.h5',\n",
    "           'w_2_custom_gray_255_mean.h5',\n",
    "           'w_2_custom_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 2nd model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('custom' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('custom' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 2nd model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "       \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (278, 5)\n",
    "            print('prediction scores :', temp[0])  # 5 score numbers\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (278,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing labels\n",
    "print(labels_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels_custom)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='autumn_r', xticks_rotation=25)\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 2nd model, Custom Dataset', fontsize=20)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_2_custom_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays of\n",
    "# Mean Image and Standard Deviation\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_custom_bgr = cv2.imread('images_to_test' + '/' + 'custom_to_test_2.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_custom_rgb = cv2.cvtColor(image_custom_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 64 by 64 pixels size\n",
    "image_custom_rgb = cv2.resize(image_custom_rgb,\n",
    "                              (64, 64),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_custom_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_custom_rgb_255 = image_custom_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_custom_rgb_255_mean = image_custom_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_custom_rgb_255_mean_std = image_custom_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_custom_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_custom_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_custom_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_custom_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_custom_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_custom_gray = cv2.cvtColor(image_custom_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_custom_gray = image_custom_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_custom_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_custom_gray_255 = image_custom_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_custom_gray_255_mean = image_custom_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_custom_gray_255_mean_std = image_custom_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_custom_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_custom_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_custom_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_custom_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_custom_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_custom_rgb_255_mean = image_custom_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_custom_rgb_255_mean_std = image_custom_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_custom_gray_255_mean = image_custom_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_custom_gray_255_mean_std = image_custom_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_custom_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_custom_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_custom_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_custom_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_custom_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_custom_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, custom_rgb_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_custom_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_custom_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, custom_rgb_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_custom_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_custom_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, custom_gray_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_custom_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_custom_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 5 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_custom[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, custom_gray_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_1_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_1_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_1_cifar10_rgb_255_mean.h5',\n",
    "           'w_1_cifar10_rgb_255_mean_std.h5',\n",
    "           'w_1_cifar10_gray_255_mean.h5',\n",
    "           'w_1_cifar10_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 1st model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('cifar10' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('cifar10' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 1st model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "            print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "      \n",
    "    \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (10000,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing labels for CIFAR-10 dataset\n",
    "# Getting Pandas dataFrame from txt file with labels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "labels_cifar10 = pd.read_csv(full_path_to_Section2 + '/' + \n",
    "                             'cifar10' + '/' + \n",
    "                             'batches.meta.txt', header=None)\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing first 5 elements of the dataFrame\n",
    "print(labels_cifar10.head())\n",
    "print()\n",
    "\n",
    "\n",
    "# Converting into Numpy array\n",
    "labels_cifar10 = np.array(labels_cifar10).flatten()\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing Numpy array with labels\n",
    "print(labels_cifar10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels_cifar10)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='Blues', xticks_rotation=35)\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 1st model, CIFAR-10 Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_1_cifar10_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Mean Image for RGB CIFAR-10 dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + \n",
    "               'mean_rgb_dataset_cifar10.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_rgb = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_rgb = np.array(mean_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for RGB CIFAR-10 dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + \n",
    "               'std_rgb_dataset_cifar10.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_rgb = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_rgb = np.array(std_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Mean Image for GRAY CIFAR-10 dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + \n",
    "               'mean_gray_dataset_cifar10.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_gray = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_gray = np.array(mean_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for GRAY CIFAR-10 dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + \n",
    "               'std_gray_dataset_cifar10.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_gray = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_gray = np.array(std_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_cifar10_bgr = cv2.imread('images_to_test' + '/' + 'cifar10_to_test_1.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_cifar10_rgb = cv2.cvtColor(image_cifar10_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 32 by 32 pixels size\n",
    "image_cifar10_rgb = cv2.resize(image_cifar10_rgb,\n",
    "                              (32, 32),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_cifar10_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_cifar10_rgb_255 = image_cifar10_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_cifar10_rgb_255_mean = image_cifar10_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_cifar10_rgb_255_mean_std = image_cifar10_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_cifar10_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_cifar10_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_cifar10_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_cifar10_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_cifar10_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_cifar10_gray = cv2.cvtColor(image_cifar10_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_cifar10_gray = image_cifar10_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_cifar10_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_cifar10_gray_255 = image_cifar10_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_cifar10_gray_255_mean = image_cifar10_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_cifar10_gray_255_mean_std = image_cifar10_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_cifar10_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_cifar10_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_cifar10_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_cifar10_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_cifar10_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_cifar10_rgb_255_mean = image_cifar10_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_cifar10_rgb_255_mean_std = image_cifar10_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_cifar10_gray_255_mean = image_cifar10_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_cifar10_gray_255_mean_std = image_cifar10_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_cifar10_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_cifar10_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_cifar10_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_cifar10_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_cifar10_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_cifar10_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st RGB model, cifar10_rgb_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_cifar10_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_cifar10_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st RGB model, cifar10_rgb_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_cifar10_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_cifar10_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, cifar10_gray_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_cifar10_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_cifar10_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, cifar10_gray_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_2_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_2_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_2_cifar10_rgb_255_mean.h5',\n",
    "           'w_2_cifar10_rgb_255_mean_std.h5',\n",
    "           'w_2_cifar10_gray_255_mean.h5',\n",
    "           'w_2_cifar10_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 2nd model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('cifar10' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('cifar10' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 2nd model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "            print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "      \n",
    "    \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (10000,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing Numpy array with labels\n",
    "print(labels_cifar10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels_cifar10)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='cool', xticks_rotation=35)\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'Blues', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 2nd model, CIFAR-10 Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_2_cifar10_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays of\n",
    "# Mean Image and Standard Deviation\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_cifar10_bgr = cv2.imread('images_to_test' + '/' + 'cifar10_to_test_2.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_cifar10_rgb = cv2.cvtColor(image_cifar10_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 32 by 32 pixels size\n",
    "image_cifar10_rgb = cv2.resize(image_cifar10_rgb,\n",
    "                              (32, 32),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_cifar10_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_cifar10_rgb_255 = image_cifar10_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_cifar10_rgb_255_mean = image_cifar10_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_cifar10_rgb_255_mean_std = image_cifar10_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_cifar10_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_cifar10_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_cifar10_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_cifar10_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_cifar10_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_cifar10_gray = cv2.cvtColor(image_cifar10_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_cifar10_gray = image_cifar10_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_cifar10_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_cifar10_gray_255 = image_cifar10_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_cifar10_gray_255_mean = image_cifar10_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_cifar10_gray_255_mean_std = image_cifar10_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_cifar10_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_cifar10_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_cifar10_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_cifar10_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_cifar10_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_cifar10_rgb_255_mean = image_cifar10_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_cifar10_rgb_255_mean_std = image_cifar10_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_cifar10_gray_255_mean = image_cifar10_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_cifar10_gray_255_mean_std = image_cifar10_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_cifar10_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_cifar10_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_cifar10_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_cifar10_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_cifar10_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_cifar10_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, cifar10_rgb_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_cifar10_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_cifar10_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, cifar10_rgb_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_cifar10_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_cifar10_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, cifar10_gray_255_mean',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_cifar10_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_cifar10_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_cifar10[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, cifar10_gray_255_mean_std',\n",
    "          show_xticks=True,\n",
    "          labels=labels_cifar10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + \n",
    "                                 'model_1_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_1_mnist_gray_255_mean.h5',\n",
    "           'w_1_mnist_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 1st model\n",
    "for i in range(2):    \n",
    "    # loading and assigning best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    model_gray[i].load_weights('mnist' + '/' + weights[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Best weights for 1st GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 1st model with all MNIST datasets in a loop\n",
    "for i in range(2):    \n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Testing RGB model with current dataset\n",
    "    temp = model_gray[i].predict(x_test)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing prediction shape and scores\n",
    "    if i == 0:\n",
    "        print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "        print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "    \n",
    "    \n",
    "    # Getting indexes of maximum values along specified axis\n",
    "    temp = np.argmax(temp, axis=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing prediction shape after convertion\n",
    "    # Showing predicted and correct indexes of classes\n",
    "    if i == 0:\n",
    "        print('prediction shape  :', temp.shape)  # (10000,)\n",
    "        print('predicted indexes :', temp[0:10])\n",
    "        print('correct indexes   :', y_test[:10])\n",
    "    \n",
    "    \n",
    "    # Calculating accuracy\n",
    "    # We compare predicted class with correct class for all input images\n",
    "    # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "    # By function 'np.mean' we calculate mean value:\n",
    "    # all_True / (all_True + all_False)\n",
    "    accuracy = np.mean(temp == y_test)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing True and False matrix\n",
    "    if i == 0:\n",
    "         print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing calculated accuracy\n",
    "    print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='Greens')\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'Blues', 'cool', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 1st model, MNIST Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_1_mnist_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Mean Image for GRAY MNIST dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + \n",
    "               'mean_gray_dataset_mnist.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_gray = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_gray = np.array(mean_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for GRAY MNIST dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + \n",
    "               'std_gray_dataset_mnist.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_gray = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_gray = np.array(std_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_mnist_bgr = cv2.imread('images_to_test' + '/' + 'mnist_to_test_1.jpg')\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_mnist_gray = cv2.cvtColor(image_mnist_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Resizing image to 28 by 28 pixels size\n",
    "image_mnist_gray = cv2.resize(image_mnist_gray,\n",
    "                              (28, 28),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_mnist_gray = image_mnist_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_mnist_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_mnist_gray_255 = image_mnist_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_mnist_gray_255_mean = image_mnist_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_mnist_gray_255_mean_std = image_mnist_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_mnist_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_mnist_gray[5, 4:9, 0])\n",
    "print('GRAY /255.0                :', image_mnist_gray_255[5, 4:9, 0])\n",
    "print('GRAY /255.0 => mean        :', image_mnist_gray_255_mean[5, 4:9, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_mnist_gray_255_mean_std[5, 4:9, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_mnist_gray_255_mean = image_mnist_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_mnist_gray_255_mean_std = image_mnist_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('GRAY /255.0 => mean        :', image_mnist_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_mnist_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_mnist_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_mnist_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, mnist_gray_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_mnist_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_mnist_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index and time\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, mnist_gray_255_mean_std',\n",
    "          show_xticks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + \n",
    "                                 'model_2_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_2_mnist_gray_255_mean.h5',\n",
    "           'w_2_mnist_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 2nd model\n",
    "for i in range(2):    \n",
    "    # loading and assigning best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    model_gray[i].load_weights('mnist' + '/' + weights[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Best weights for 2nd GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 2nd model with all MNIST datasets in a loop\n",
    "for i in range(2):    \n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Testing RGB model with current dataset\n",
    "    temp = model_gray[i].predict(x_test)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing prediction shape and scores\n",
    "    if i == 0:\n",
    "        print('prediction shape  :', temp.shape)  # (10000, 10)\n",
    "        print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "    \n",
    "    \n",
    "    # Getting indexes of maximum values along specified axis\n",
    "    temp = np.argmax(temp, axis=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing prediction shape after convertion\n",
    "    # Showing predicted and correct indexes of classes\n",
    "    if i == 0:\n",
    "        print('prediction shape  :', temp.shape)  # (10000,)\n",
    "        print('predicted indexes :', temp[0:10])\n",
    "        print('correct indexes   :', y_test[:10])\n",
    "    \n",
    "    \n",
    "    # Calculating accuracy\n",
    "    # We compare predicted class with correct class for all input images\n",
    "    # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "    # By function 'np.mean' we calculate mean value:\n",
    "    # all_True / (all_True + all_False)\n",
    "    accuracy = np.mean(temp == y_test)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing True and False matrix\n",
    "    if i == 0:\n",
    "         print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing calculated accuracy\n",
    "    print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class\n",
    "# Each column represents a number of predicted class\n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='Greys')\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'Blues', 'cool', 'Greens', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 2nd model, MNIST Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_2_mnist_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays of\n",
    "# Mean Image and Standard Deviation\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_mnist_bgr = cv2.imread('images_to_test' + '/' + 'mnist_to_test_2.jpg')\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_mnist_gray = cv2.cvtColor(image_mnist_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Resizing image to 28 by 28 pixels size\n",
    "image_mnist_gray = cv2.resize(image_mnist_gray,\n",
    "                              (28, 28),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_mnist_gray = image_mnist_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_mnist_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_mnist_gray_255 = image_mnist_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_mnist_gray_255_mean = image_mnist_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_mnist_gray_255_mean_std = image_mnist_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_mnist_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_mnist_gray[5, 4:9, 0])\n",
    "print('GRAY /255.0                :', image_mnist_gray_255[5, 4:9, 0])\n",
    "print('GRAY /255.0 => mean        :', image_mnist_gray_255_mean[5, 4:9, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_mnist_gray_255_mean_std[5, 4:9, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_mnist_gray_255_mean = image_mnist_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_mnist_gray_255_mean_std = image_mnist_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('GRAY /255.0 => mean        :', image_mnist_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_mnist_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_mnist_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_mnist_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, mnist_gray_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_mnist_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_mnist_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 10 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 0:5])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, mnist_gray_255_mean_std',\n",
    "          show_xticks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_1_ts_rgb.h5'))\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_1_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_1_ts_rgb_255_mean.h5',\n",
    "           'w_1_ts_rgb_255_mean_std.h5',\n",
    "           'w_1_ts_gray_255_mean.h5',\n",
    "           'w_1_ts_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 1st model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('ts' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('ts' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 1st GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 1st model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (3111, 43)\n",
    "            print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "      \n",
    "    \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (3111,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing labels for Traffic Signs dataset\n",
    "# Getting Pandas dataFrame with labels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "labels_ts = pd.read_csv(full_path_to_Section3 + '/' + 'classes_names.csv', sep=',')\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing first 5 elements of the dataFrame\n",
    "print(labels_ts.head())\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class's name of the 1st element\n",
    "print(labels_ts.loc[0, 'SignName'])\n",
    "print()\n",
    "\n",
    "\n",
    "# Converting into Numpy array\n",
    "labels_ts = np.array(labels_ts.loc[:, 'SignName']).flatten()\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing size of Numpy array\n",
    "# Showing all elements of Numpy array\n",
    "print('Total number of labels:', labels_ts.size)\n",
    "print()\n",
    "print(labels_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='PuRd')\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "\n",
    "# Setting fontsize for xlabels and ylabels\n",
    "plt.xlabel('Predicted label', fontsize=18)\n",
    "plt.ylabel('True label', fontsize=18)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 1st model, Traffic Signs Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_1_ts_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Mean Image for RGB Traffic Signs dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + 'mean_rgb_dataset_ts.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_rgb = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_rgb = np.array(mean_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for RGB Traffic Signs dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + 'std_rgb_dataset_ts.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_rgb = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_rgb = np.array(std_rgb)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Mean Image for GRAY Traffic Signs dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + 'mean_gray_dataset_ts.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Mean Image\n",
    "    # Saving it into new variable\n",
    "    mean_gray = f['mean']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    mean_gray = np.array(mean_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Opening saved Standard Deviation for GRAY Traffic Signs dataset\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + 'std_gray_dataset_ts.hdf5', 'r') as f:\n",
    "    # Extracting saved array for Standard Deviation\n",
    "    # Saving it into new variable\n",
    "    std_gray = f['std']  # HDF5 dataset\n",
    "    # Converting it into Numpy array\n",
    "    std_gray = np.array(std_gray)  # Numpy arrays\n",
    "\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_ts_bgr = cv2.imread('images_to_test' + '/' + 'ts_to_test_1.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_ts_rgb = cv2.cvtColor(image_ts_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 32 by 32 pixels size\n",
    "image_ts_rgb = cv2.resize(image_ts_rgb,\n",
    "                              (48, 48),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_ts_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_ts_rgb_255 = image_ts_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_ts_rgb_255_mean = image_ts_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_ts_rgb_255_mean_std = image_ts_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_ts_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_ts_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_ts_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_ts_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_ts_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_ts_gray = cv2.cvtColor(image_ts_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_ts_gray = image_ts_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_ts_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_ts_gray_255 = image_ts_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_ts_gray_255_mean = image_ts_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_ts_gray_255_mean_std = image_ts_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_ts_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_ts_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_ts_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_ts_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_ts_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_ts_rgb_255_mean = image_ts_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_ts_rgb_255_mean_std = image_ts_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_ts_gray_255_mean = image_ts_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_ts_gray_255_mean_std = image_ts_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_ts_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_ts_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_ts_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_ts_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_ts_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 10:15])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st RGB model, ts_rgb_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_ts_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 10:15])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st RGB model, ts_rgb_255_mean_std',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_ts_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 10:15])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, ts_gray_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_ts_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 10:15])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='1st GRAY model, ts_gray_255_mean_std',\n",
    "          show_xticks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_2_ts_rgb.h5'))\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_2_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 2: Loading and assigning best weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with weights' names\n",
    "weights = ['w_2_ts_rgb_255_mean.h5',\n",
    "           'w_2_ts_rgb_255_mean_std.h5',\n",
    "           'w_2_ts_gray_255_mean.h5',\n",
    "           'w_2_ts_gray_255_mean_std.h5']\n",
    "\n",
    "\n",
    "# Loading best weights for 2nd model\n",
    "for i in range(4):    \n",
    "    # Checking if it is RGB model\n",
    "    if i <= 1:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_rgb[i].load_weights('ts' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd RGB model are loaded and assigned  : ', weights[i])\n",
    "    \n",
    "    # Checking if it is GRAY model\n",
    "    elif i >= 2:\n",
    "        # loading and assigning best weights\n",
    "        # (!) On Windows, it might need to change\n",
    "        # this: + '/' +\n",
    "        # to this: + '\\' +\n",
    "        # or to this: + '\\\\' +\n",
    "        model_gray[i-2].load_weights('ts' + '/' + weights[i])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        print('Best weights for 2nd GRAY model are loaded and assigned : ', weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 3: Predicting with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining variable to identify the best model\n",
    "accuracy_best = 0\n",
    "\n",
    "\n",
    "# Testing 2nd model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for testing by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_test = f['x_test']  # HDF5 dataset\n",
    "        y_test = f['y_test']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_test = np.array(x_test)  # Numpy arrays\n",
    "        y_test = np.array(y_test)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Dataset is opened :', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    # Showing shapes of loaded arrays\n",
    "    if i == 0:\n",
    "        print('x_test shape      :', x_test.shape)\n",
    "        print('y_test shape      :', y_test.shape)\n",
    "    \n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Testing RGB model with current dataset\n",
    "        temp = model_rgb[i].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape and scores\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (3111, 43)\n",
    "            print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n",
    "      \n",
    "    \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing prediction shape after convertion\n",
    "        # Showing predicted and correct indexes of classes\n",
    "        if i == 0:\n",
    "            print('prediction shape  :', temp.shape)  # (3111,)\n",
    "            print('predicted indexes :', temp[0:10])\n",
    "            print('correct indexes   :', y_test[:10])\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing True and False matrix\n",
    "        if i == 0:\n",
    "            print('T and F matrix    :', (temp == y_test)[0:10])\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Testing GRAY model with current dataset\n",
    "        temp = model_gray[i-2].predict(x_test)\n",
    "        \n",
    "        \n",
    "        # Getting indexes of maximum values along specified axis\n",
    "        temp = np.argmax(temp, axis=1)\n",
    "        \n",
    "        \n",
    "        # Calculating accuracy\n",
    "        # We compare predicted class with correct class for all input images\n",
    "        # By saying 'temp == y_test' we create Numpy array with True and False values\n",
    "        # By function 'np.mean' we calculate mean value:\n",
    "        # all_True / (all_True + all_False)\n",
    "        accuracy = np.mean(temp == y_test)\n",
    "        \n",
    "        \n",
    "        # Check point\n",
    "        # Showing calculated accuracy\n",
    "        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Identifying the best model\n",
    "    # Saving predicted indexes of the best model\n",
    "    if accuracy > accuracy_best:\n",
    "        # Updating value of the best accuracy\n",
    "        accuracy_best = accuracy\n",
    "        \n",
    "        # Saving predicted indexes of the best model into array\n",
    "        # Updating array with predicted indexes of the best model\n",
    "        y_predicted_best = temp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 4: Classification report & Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TP (True Positive)** is a number of **right predictions** that are **correct**  \n",
    "when label is **True** *and* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **TN (True Negative)** is a number of **right predictions** that are **incorrect**  \n",
    "when label is **False** *and* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **FP (False Positive)** is a number of **not right predictions** that are **incorrect**  \n",
    "when label is **False** *but* predicted as **True**  \n",
    "  \n",
    "  \n",
    "- **FN (False Negative)** is a number of **not right predictions** that are **correct**  \n",
    "when label is **True** *but* predicted as **False**  \n",
    "  \n",
    "  \n",
    "- **Precision**  is an accuracy of positive predictions  \n",
    "Precision represents **percent of correct predictions**  \n",
    "In other words, it is **ability not to label** an image **as positive** that is actually **negative**   \n",
    "Precision is calculated by following equation:  \n",
    "Precision = TP / (TP + FP)  \n",
    "  \n",
    "  \n",
    "- **Recall**  is a fraction of positive predictions among all True samples  \n",
    "In other words, it is **ability to find all positive samples**  \n",
    "Recall is calculated by following equation:  \n",
    "Recall = TP / (TP + FN)  \n",
    "  \n",
    "  \n",
    "- **F1-score**  is a so called **weighted harmonic mean of the Precision and Recall**  \n",
    "F1-score also known as balanced F-score or F-measure,  \n",
    "as it incorporates Precision and Recall into computation,  \n",
    "and, therefore, contributions of Precision and Recall to F1-score are equal  \n",
    "F1-score reaches its best value at 1 and worst score at 0  \n",
    "F1-score is calculated by following equation:  \n",
    "F1-score = 2 * (Recall * Precision) / (Recall + Precision)  \n",
    "  \n",
    "  \n",
    "- **Support** is a number of occurrences of each class in a dataset  \n",
    "  \n",
    "  \n",
    "- **Accuracy** is a global accuracy of entire classifier  \n",
    "Accuracy is calculated by following equation:  \n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)  \n",
    "(all correct / all)  \n",
    "\n",
    "  \n",
    "- **macro avg** calculates the mean of the metrics,   \n",
    "giving equal weight to each class  \n",
    "  \n",
    "  \n",
    "- **weighted avg** calculates the weighted mean of the metrics  \n",
    "It takes into account imbalance of samples' number for every class  \n",
    "It weights every metric by occurrences of each class in a dataset  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the main classification metrics of the best model\n",
    "print(classification_report(y_test, y_predicted_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix is a two dimensional matrix that visualizes the performance,\n",
    "# and makes it easy to see confusion between classes,\n",
    "# by providing a picture of interrelation\n",
    "\n",
    "# Each row represents a number of actual class  \n",
    "# Each column represents a number of predicted class  \n",
    "\n",
    "\n",
    "# Computing confusion matrix to evaluate accuracy of classification\n",
    "c_m = confusion_matrix(y_test, y_predicted_best)\n",
    "\n",
    "# Showing confusion matrix in form of Numpy array\n",
    "print(c_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing size of Numpy array\n",
    "# Showing all elements of Numpy array\n",
    "print('Total number of labels:', labels_ts.size)\n",
    "print()\n",
    "print(labels_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# Implementing visualization of confusion matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m)\n",
    "\n",
    "\n",
    "# Plotting confusion matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='copper_r')\n",
    "# Other possible options for colour map are:\n",
    "# 'OrRd', 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "\n",
    "# Setting fontsize for xlabels and ylabels\n",
    "plt.xlabel('Predicted label', fontsize=18)\n",
    "plt.ylabel('True label', fontsize=18)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix: 2nd model, Traffic Signs Dataset', fontsize=18)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix_model_2_ts_dataset.png', transparent=True, dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 5: Testing on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check points\n",
    "# Showing shapes of loaded Numpy arrays of\n",
    "# Mean Image and Standard Deviation\n",
    "print('RGB Mean Image          :', mean_rgb.shape)\n",
    "print('RGB Standard Deviation  :', std_rgb.shape)\n",
    "print('GRAY Mean Image         :', mean_gray.shape)\n",
    "print('GRAY Standard Deviation :', std_gray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (2.5, 2.5)\n",
    "\n",
    "\n",
    "\n",
    "# Reading image by OpenCV library\n",
    "# In this way image is opened already as Numpy array\n",
    "# (!) OpenCV by default reads images in BGR order of channels\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "image_ts_bgr = cv2.imread('images_to_test' + '/' + 'ts_to_test_2.jpg')\n",
    "\n",
    "# Swapping channels from BGR to RGB by OpenCV function\n",
    "image_ts_rgb = cv2.cvtColor(image_ts_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resizing image to 32 by 32 pixels size\n",
    "image_ts_rgb = cv2.resize(image_ts_rgb,\n",
    "                              (48, 48),\n",
    "                              interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Check point\n",
    "# Showing loaded and resized image\n",
    "plt.imshow(image_ts_rgb)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_ts_rgb_255 = image_ts_rgb / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_ts_rgb_255_mean = image_ts_rgb_255 - mean_rgb\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_ts_rgb_255_mean_std = image_ts_rgb_255_mean / std_rgb\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with RGB image\n",
    "# Showing some pixels' values\n",
    "print('Shape of RGB image         :', image_ts_rgb.shape)\n",
    "print('Pixels of RGB image        :', image_ts_rgb[:5, 0, 0])\n",
    "print('RGB /255.0                 :', image_ts_rgb_255[:5, 0, 0])\n",
    "print('RGB /255.0 => mean         :', image_ts_rgb_255_mean[:5, 0, 0])\n",
    "print('RGB /255.0 => mean => std  :', image_ts_rgb_255_mean_std[:5, 0, 0])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# Converting image to GRAY by OpenCV function\n",
    "image_ts_gray = cv2.cvtColor(image_ts_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extending dimension from (height, width) to (height, width, one channel)\n",
    "image_ts_gray = image_ts_gray[:, :, np.newaxis]\n",
    "\n",
    "# Check point\n",
    "# Showing converted into GRAY image\n",
    "plt.imshow(image_ts_gray, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Implementing normalization by dividing image's pixels on 255.0\n",
    "image_ts_gray_255 = image_ts_gray / 255.0\n",
    "\n",
    "# Implementing normalization by subtracting Mean Image\n",
    "image_ts_gray_255_mean = image_ts_gray_255 - mean_gray\n",
    "\n",
    "# Implementing preprocessing by dividing on Standard Deviation\n",
    "image_ts_gray_255_mean_std = image_ts_gray_255_mean / std_gray\n",
    "\n",
    "# Check points\n",
    "# Showing shape of Numpy array with GRAY image\n",
    "# Showing some pixels' values\n",
    "print('Shape of GRAY image        :', image_ts_gray.shape)\n",
    "print('Pixels of GRAY image       :', image_ts_gray[:5, 0, 0])\n",
    "print('GRAY /255.0                :', image_ts_gray_255[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean        :', image_ts_gray_255_mean[:5, 0, 0])\n",
    "print('GRAY /255.0 => mean => std :', image_ts_gray_255_mean_std[:5, 0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending dimension from (height, width, channels) to (1, height, width, channels)\n",
    "image_ts_rgb_255_mean = image_ts_rgb_255_mean[np.newaxis, :, :, :]\n",
    "image_ts_rgb_255_mean_std = image_ts_rgb_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "image_ts_gray_255_mean = image_ts_gray_255_mean[np.newaxis, :, :, :]\n",
    "image_ts_gray_255_mean_std = image_ts_gray_255_mean_std[np.newaxis, :, :, :]\n",
    "\n",
    "# Check points\n",
    "# Showing shapes of extended Numpy arrays\n",
    "print('RGB /255.0 => mean         :', image_ts_rgb_255_mean.shape)\n",
    "print('RGB /255.0 => mean => std  :', image_ts_rgb_255_mean_std.shape)\n",
    "print()\n",
    "print('GRAY /255.0 => mean        :', image_ts_gray_255_mean.shape)\n",
    "print('GRAY /255.0 => mean => std :', image_ts_gray_255_mean_std.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing information about created function\n",
    "print(help(bar_chart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[0].predict(image_ts_rgb_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 35:40])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, ts_rgb_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_rgb[1].predict(image_ts_rgb_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 35:40])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd RGB model, ts_rgb_255_mean_std',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[0].predict(image_ts_gray_255_mean)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 35:40])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, ts_gray_255_mean',\n",
    "          show_xticks=False)\n",
    "\n",
    "\n",
    "\n",
    "# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean_std.hdf5\n",
    "# Input image is preprocessed in the same way\n",
    "# Measuring classification time\n",
    "start = timer()\n",
    "scores = model_gray[1].predict(image_ts_gray_255_mean_std)\n",
    "end = timer()\n",
    "\n",
    "# Scores are given as 43 numbers of predictions for each class\n",
    "# Getting only one class with maximum value\n",
    "prediction = np.argmax(scores)\n",
    "\n",
    "# Check points\n",
    "# Showing scores shape and values\n",
    "# Printing class index, label and time\n",
    "print()\n",
    "print('Scores shape        :', scores.shape)\n",
    "print('Scores values       :', scores[0, 35:40])\n",
    "print('Scores sum          :', scores[0].sum())\n",
    "print('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\n",
    "print('Class index         :', prediction)\n",
    "print('Label               :', labels_ts[prediction])\n",
    "print('Time                : {0:.5f}'.format(end - start))\n",
    "\n",
    "# Plotting bar chart with scores values\n",
    "bar_chart(scores[0],\n",
    "          bar_title='2nd GRAY model, ts_gray_255_mean_std',\n",
    "          show_xticks=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'np.argmax':  \n",
    "**print(help(np.argmax))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://numpy.org/doc/stable/reference/generated/numpy.argmax.html  \n",
    "  \n",
    "  \n",
    "To get more details for usage of 'classification_report':  \n",
    "**print(help(classification_report))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html  \n",
    "  \n",
    "  \n",
    "To get more details for usage of 'confusion_matrix':  \n",
    "**print(help(confusion_matrix))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://www.sklearn.org/modules/generated/sklearn.metrics.confusion_matrix.html  \n",
    "  \n",
    "  \n",
    "To get more details for usage of 'plt.colormaps()':  \n",
    "**print(help(plt.colormaps()))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " -  https://matplotlib.org/api/pyplot_summary.html?highlight=colormaps#matplotlib.pyplot.colormaps  \n",
    "  \n",
    "  \n",
    "To get more details for usage of 'cv2.resize':  \n",
    "**print(help(cv2.resize))**\n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://docs.opencv.org/4.3.0/da/d54/group__imgproc__transform.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "print(help(Sequential.load_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "print(help(Sequential.predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(np.newaxis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(np.argmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(classification_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(help(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.colormaps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(plt.rcParams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(plt.savefig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plt.bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(plt.xticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(cv2.resize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
